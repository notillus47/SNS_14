from bs4 import BeautifulSoup
from selenium import webdriver
import time
import sys
import re
import math
import numpy 
import pandas as pd   
import xlwt 
import random
import os
import urllib.request
import urllib
import win32com.client as win32
import win32api

print("=" *70)
print("지마켓 베스트셀러 상품의 정보를 수집합니다.")
print("=" *70)

query_txt='Gmarket Best 100'
query_url='http://corners.gmarket.co.kr/Bestsellers'

f_dir = input("1. 파일을 저장할 폴더명만 쓰세요(예:c:\\temp\\) : ")
print("\n")

now = time.localtime()
s = '%04d-%02d-%02d-%02d-%02d-%02d' % (now.tm_year, now.tm_mon, now.tm_mday, now.tm_hour, now.tm_min, now.tm_sec)

os.makedirs(f_dir+s+'-'+query_txt)
os.chdir(f_dir+s+'-'+query_txt)

ff_dir=f_dir+s+'-'+query_txt+'-'
ff_name=f_dir+s+'-'+query_txt+'-'+'\\'+s+'-'+query_txt+'-'+'.txt'
fc_name=f_dir+s+'-'+query_txt+'-'+'\\'+s+'-'+query_txt+'-'+'.csv'
fx_name=f_dir+s+'-'+query_txt+'-'+'\\'+s+'-'+query_txt+'-'+'.xls'

s_time = time.time( )

path = 'C:/tmp/chromedriver.exe'
driver = webdriver.Chrome(path)
    
driver.get(query_url)
time.sleep(5)

driver.execute_script("window.scrollBy(0,9300);")
time.sleep(1)

bmp_map = dict.fromkeys(range(0x10000, sys.maxunicode + 1), 0xfffd)

html = driver.page_source
soup = BeautifulSoup(html, 'html.parser')

img_src2=[]   # 이미지 URL 저장변수
file_no = 0
slist=[]

search = soup.select('#gBestWrap-div > #best-list-div')
slist = search[0].find_all('li')

ranking = []
explain = []
o_price = []
s_price = []
discount = []

for li in slist:
    try :
        photo = li.find('img')['src']
    except AttributeError :
        continue
    file_no += 1

    urllib.request.urlretrieve(photo,str(file_no)+'.jpg')
    time.sleep(1)

for li in slist:
    f=open(ff_name, 'a', encoding='UTF-8-sig')
    f.write("-"*30 + "\n")
    
    print("-"*70)
    ranking1 = li.find('p').get_text()
    f.write("1. 판매순위 : "+ranking+"\n")
    print("1. 판매순위 : "+ranking+"\n")
    ranking.append(ranking1)
    
    expalain1 = li.find('a', class_='itemname').get_text()
    f.write("2. 제품소개 : "+ explain1 + "\n")
    print("2. 제품소개 : "+ explain1 + "\n")
    expalin.append(explain1)
    
    o_price1 = li.find('div', class_='o-price').get_text()
    f.write("3. 원래가격 : " + o_price1 + "\n")
    print("3. 원래가격 : " + o_price1 + "\n")
    o_price.append(o_price1)
    
    s_price1 = li.find('div', class_='s-price').get_text()
    f.write("4. 판매가격 : " + s-price1 + "\n")
    print("4. 판매가격 : " + s-price1 + "\n")
    s_price.append(s_price1)
    
    discount1 = li.find('span', class_='sale').get_text()
    f.write("5. 할인율 : " + discount1 + "\n")
    print("5. 할인율 : " + discount1 + "\n")
    discount.append(discount1)
    


result = pd.DataFrame()
result['판매순위']=ranking
result['제품소개']=pd.Series(explain)
result['원래가격']=pd.Series(o_price)
result['판매가격']=pd.Series(s_price)
result['할인율']=pd.Series(discount)

excel = win32.gencache.EnsureDispatch('Excel.Application')
wb = excel.Workbooks.Open(fx_name)
sheet = wb.ActiveSheet
sheet.Columns(3).ColumnWidth = 30
row_cnt = cnt+1
sheet.Rows("2:%s" %row_cnt).RowHeight = 120

ws = wb.Sheets("Sheet1")
col_name2=[]
file_name2=[]

result.to_csv(fc_name,encoding="utf-8-sig",index=True)
result.to_excel(fx_name ,index=True)
